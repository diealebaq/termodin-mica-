# Proyecto: Primera y Segunda Ley de la TermodinÃ¡mica

Este repositorio contiene una explicaciÃ³n completa, organizada y entendible de las dos leyes fundamentales de la termodinÃ¡mica.

## Contenido del proyecto

- [Primera Ley de la TermodinÃ¡mica](primera_ley.md)
- [Segunda Ley de la TermodinÃ¡mica](segunda_ley.md)
- [Ejemplos y aplicaciones](ejemplos.md)

## Objetivo del repositorio
Servir como material acadÃ©mico para comprender:
- ConservaciÃ³n de energÃ­a
- Transferencia de calor y trabajo
- EntropÃ­a
- Irreversibilidad
- MÃ¡quinas tÃ©rmicas y eficiencia
# Primera Ley de la TermodinÃ¡mica

La Primera Ley establece que:

**La energÃ­a no se crea ni se destruye, solo se transforma.**

# DefiniciÃ³n HistÃ³rica de EntropÃ­a

La entropÃ­a es una funciÃ³n de estado cuya variaciÃ³n se define clÃ¡sicamente mediante la relaciÃ³n dS =Î´Qrevâ€‹â€‹/T, que expresa cÃ³mo cambia la entropÃ­a cuando un sistema intercambia calor de manera reversible a una temperatura dada. SegÃºn la interpretaciÃ³n presentada en el texto, esta magnitud puede entenderse como una medida del grado de dispersiÃ³n o distribuciÃ³n de la energÃ­a en el sistema: cuanto mÃ¡s extensamente puede repartirse la energÃ­a entre los distintos estados posibles bajo condiciones macroscÃ³picas fijas, mayor es la entropÃ­a. AsÃ­, la formulaciÃ³n clÃ¡sica describe cuantitativamente el cambio de entropÃ­a en tÃ©rminos de calor reversible, mientras que su significado fÃ­sico se relaciona con la tendencia natural de la energÃ­a a difundirse dentro del sistema.

## Clausius definio la entropÃ­a como:

                                                                 dS =Î´Qrevâ€‹â€‹/T

Clausius introdujo por primera vez la palabra entropÃ­a, para describir una magnitud de estado asociada al calor, donde:


-  DS= Cambio infinitesimal de entropÃ­a

- Î´Qrev= Calor intercambiado de forma irreversible

- T= Temperatura absoluta

Para Clausius, la entropÃ­a es la magnitud que permite cuantificar el cambio del calor cuando este se reparte en un sistema a una temperatura dada durante un proceso reversible. Ese reparto del calor implica una disminuciÃ³n en su capacidad para producir trabajo, lo que constituye la esencia de la irreversibilidad.

## Ejemplos donde se aplica

1. ExpansiÃ³n libre de un gas

1 mol de gas ideal se encuentra en un recipiente con volumen inicial 
ð‘‰ð‘–=2L y se expande libremente hasta ð‘‰ð‘“=6L a temperatura constante ð‘‡=300K
No hay intercambio de calor con el entorno (expansiÃ³n libre = adiabÃ¡tica e irreversible).


                                                                Î”S=nRln(Vi/â€‹Vf)â€‹â€‹


Î”Sâ‰ˆ8.314Ã—1.0986â‰ˆ9.13J/K

La energÃ­a cinÃ©tica de las molÃ©culas se distribuye ahora en un volumen mayor â†’ mayor dispersiÃ³n de energÃ­a.

La entropÃ­a aumenta Î”ð‘†>0, reflejando irreversibilidad segÃºn Clausius.

2. Transferencia de calor entre cuerpos a distinta temperatura

Un bloque caliente de metal (ð¶â„Ž=200J/K ,ð‘‡â„Ž=400â€‰K) cede calor a un bloque frÃ­o de agua (ð¶ð‘=100J/K, ð‘‡ð‘=300K) hasta que alcanzan equilibrio tÃ©rmico.

Tf=366.67K

Cambio de entroÃ­a en cada cuerpo:

                                                                Î”S=Cln(Ti/â€‹Tf)


Bloque caliente: Î”Sh=200ln(366.67/400)â€‹=200ln 0.9167â‰ˆ200Ã—(âˆ’0.087)â‰ˆâˆ’17.4J/Kâ€‹â€‹

Bloque frÃ­o: Î”Scâ€‹=100ln(366.67/â€‹300)=100ln 1.222â‰ˆ100Ã—0.200â‰ˆ20.0J/K

EntropÃ­a total= Î”Stotalâ€‹=Î”Shâ€‹+Î”Scâ€‹=âˆ’17.4+20.0â‰ˆ2.6J/K

La entropÃ­a total aumenta, cumpliendo la segunda ley de termodinÃ¡mica

El calor se dispersa del bloque caliente al frÃ­o, disminuyendo la energÃ­a utilizable para trabajo.

Representa la dispersiÃ³n de energÃ­a tÃ©rmica hacia estados menos concentrados y la irreversibilidad del proceso.
## Referencias

1. https://espanol.libretexts.org/Bookshelves/Quimica/Qu%C3%ADmica_General/Libro%3A_Chem1_%28Inferior%29/15%3A_Termodin%C3%A1mica_de_Equilibrios_Qu%C3%ADmicos/15.03%3A_La_Segunda_Ley_de_la_Termodin%C3%A1mica
2. https://www.britannica.com/science/entropy-physics

## Definiciones alternativas y modernas de entropÃ­a
---
### EntropÃ­a como propiedad de la termodinÃ¡mica 

***Se denomina entropÃ­a a la magnitud que indica la energÃ­a que no puede realizar un trabajo Ãºtil en un proceso termodinÃ¡mico.***

La entropÃ­a se define como la medida de la dispersiÃ³n de la energÃ­a en un sistema. Cuanto mayor es la entropÃ­a de un sistema, mayor es su grado de aleatoridad. Se puede pensar en la entropÃ­a como una medida de la cantidad de posibles configuraciones que puede tener un sistema, considerando todas las posibles combinaciones de partÃ­culas y energÃ­a. Se denota con la letra S y se expresa en unidades de energÃ­a dividida por temperatura, generalmente en julios por kelvin (J/K).

La entropÃ­a fÃ­sica, en su forma clÃ¡sica, es definida por la ecuaciÃ³n propuesta por Rudolf Clausius:

*dS=dQ/T*

Esta es una magnitud termodinÃ¡mica definida originalmente como criterio para predecir la evoluciÃ³n de los sistemas termodinÃ¡micos. En todo proceso irreversible, el desorden del sistema aumenta y por lo tanto, la entropÃ­a aumenta. Si el proceso es reversible, la variaciÃ³n de entropÃ­a es nula.

La entropÃ­a de un sistema es una funciÃ³n de estado de carÃ¡cter extensivo. El valor de esta magnitud fÃ­sica, en un sistema aislado, crece en el transcurso de un proceso que se da de forma natural. El concepto de entropÃ­a describe cÃ³mo es de irreversible un sistema termodinÃ¡mico.

En resumen, es un concepto fundamental en la termodinÃ¡mica que se utiliza para medir la dispersiÃ³n de la energÃ­a en un sistema y esta afecta la capacidad de un sistema para realizar trabajo Ãºtil.

**Aplicaciones:**

- Podemos encontrar la aplicaciÃ³n de la entropÃ­a en las reacciones quÃ­micas. La variaciÃ³n de entropÃ­a nos muestra la variaciÃ³n del orden molecular ocurrido en una reacciÃ³n quÃ­mica. Si el incremento de entropÃ­a es positivo, los productos presentan un mayor desorden molecular (mayor entropÃ­a) que los reactivos. En cambio, cuando el incremento es negativo, los productos son mÃ¡s ordenados. Hay una relaciÃ³n entre la entropÃ­a y la espontaneidad de una reacciÃ³n quÃ­mica, que viene dada por la energÃ­a libre de Gibbs.

---
**Referencias:** 
1. https://solar-energia.net/termodinamica/propiedades-termodinamicas/entropia
2. https://conceptos.es/entropia-en-termodinamica
3. https://www.quimica.es/enciclopedia/Entrop%C3%ADa_%28termodin%C3%A1mica%29.html#Evidencias

---
### EntropÃ­a como calidad de informaciÃ³n
*"La entropÃ­a en la teorÃ­a mide la incertidumbre de los resultados y se aplica en la codificaciÃ³n para optimizar la comprensiÃ³n y transmisiÃ³n de datos."*

La entropÃ­a en la teorÃ­a de la informaciÃ³n mide la incertidumbre o la cantidad de informaciÃ³n contenida en fuentes de datos. Una mayor entropÃ­a indica una mayor incertidumbre y variabilidad de los datos. 

En codificaciÃ³n, se busca minimizar la longitud promedio de los cÃ³digos mediante comprensiÃ³n, directamente influenciada por la entropÃ­a.

**Algunos esquemas de codificaciÃ³n importantes incluyen:**

- **CodificaciÃ³n Huffman:** Utiliza frecuencias de apariciÃ³n de los sÃ­mbolos para crear un Ã¡rbol binario que asigna cÃ³digos mÃ¡s cortos a los sÃ­mbolos mÃ¡s comunes y cÃ³digos mÃ¡s largos a los menos comunes.

- **CodificaciÃ³n Shannon-Fano:** Divide los sÃ­mbolos en grupos basados en sus probabilidades y asigna cÃ³digos de manera recursiva, asegurando que los sÃ­mbolos mÃ¡s probables tengan cÃ³digos mÃ¡s cortos.

- **CodificaciÃ³n AritmÃ©tica:** Representa un mensaje como un nÃºmero real en el intervalo [0,1), asignando subintervalos a cada sÃ­mbolo basado en su probabilidad, permitiendo una compresiÃ³n cercana a la entropÃ­a teÃ³rica.
compresiÃ³n cercana a la entropÃ­a teÃ³rica.

**Aplicaciones PrÃ¡cticas**

El uso de la entropÃ­a y mÃ©todos de codificaciÃ³n eficientes es crÃ­tico en diversas aplicaciones prÃ¡cticas, como:

- **CompresiÃ³n de datos:** Algoritmos como ZIP y JPEG utilizan tÃ©cnicas basadas en la entropÃ­a para reducir el tamaÃ±o de archivos y mejorar la eficiencia del almacenamiento y la transmisiÃ³n de datos.

- **TransmisiÃ³n de informaciÃ³n:** En sistemas de comunicaciÃ³n, minimizar la redundancia y optimizar la codificaciÃ³n es esencial para maximizar la tasa de transmisiÃ³n y reducir errores.

- **CriptografÃ­a:** La entropÃ­a se utiliza para evaluar la seguridad de sistemas criptogrÃ¡ficos, garantizando que las claves y mensajes sean lo suficientemente impredecibles.

---
**Referencias:** 

1. https://www.thermal-engineering.org/es/la-entropia-en-la-teoria-de-la-informacion-y-la-codificacion/

---
### EntropÃ­a como entrelazamiento de la informaciÃ³n 

La entropÃ­a de entrelazamiento es una medida de cuÃ¡nta informaciÃ³n se comparte entre diferentes partes de un sistema. Cuando dos sistemas estÃ¡n entrelazados, conocer el estado de uno nos da informaciÃ³n sobre el estado del otro. En nuestro caso, consideramos un sistema central (llamÃ©moslo sistema-A) y un sistema de baÃ±o mÃ¡s grande (sistema-B). Estos dos sistemas estÃ¡n conectados, y sus tamaÃ±os pueden cambiar mientras que el tamaÃ±o total se mantiene fijo.

A medida que varÃ­a el tamaÃ±o de estos sistemas, sus propiedades de entrelazamiento tambiÃ©n cambian. Las leyes de conservaciÃ³n de la energÃ­a guÃ­an estas variaciones, lo que significa que la energÃ­a se equilibra entre ambos sistemas.

Cuando exploramos la entropÃ­a del sistema de baÃ±o en relaciÃ³n con el sistema central, surgen dos conceptos importantes: "islas" e "icebergs".

- Islas se refieren a regiones especÃ­ficas dentro del baÃ±o que contribuyen a la entropÃ­a total. Estas regiones estÃ¡n generalmente separadas del sistema principal, pero juegan un papel crucial en cÃ³mo se comparte la informaciÃ³n.
- Icebergs son considerados como contribuciones mÃ¡s pequeÃ±as a la entropÃ­a que, aunque no sean tan significativas por sÃ­ solas, juntas forman una parte importante de la entropÃ­a total.
  
Tanto las islas como los icebergs ayudan a crear una comprensiÃ³n mÃ¡s completa de cÃ³mo opera el entrelazamiento dentro de estos sistemas.

La interacciÃ³n entre el sistema-A y el sistema-B puede cambiar dependiendo de sus tamaÃ±os individuales. Cuando el sistema-B es significativamente mÃ¡s grande que el sistema-A, la relaciÃ³n entre sus entropÃ­as se vuelve mÃ¡s clara. En casos donde el sistema-B es pequeÃ±o, las contribuciones de las islas y los icebergs pueden desconectarse, llevando a valores de entropÃ­a calculados de manera independiente.

Sin embargo, a medida que los sistemas crecen y varÃ­an en tamaÃ±o, su entrelazamiento tambiÃ©n se vuelve estrechamente vinculado. La entropÃ­a del baÃ±o puede verse influenciada significativamente en funciÃ³n de cuÃ¡n grande es el sistema-A, reforzando la idea de conservaciÃ³n de la entropÃ­a local.

Para calcular la entropÃ­a de entrelazamiento, primero se calcula la matriz densidad reducida para uno de los subsistemas, digamos A:

*ÏA=TrB(|ÏˆABâŸ©âŸ¨ÏˆAB|)*

Luego, se computa la entropÃ­a de von Neumann de ÏA:

*SA=â€“Tr(ÏAlogÏA)*

Esta cantidad, SA, es la entropÃ­a de entrelazamiento del sistema conjunto AB. En sistemas bipartitos puramente entrelazados, la entropÃ­a de entrelazamiento es mÃ¡xima, indicando una fuerte correlaciÃ³n cuÃ¡ntica.

**Aplicaciones**

- Un ejemplo clÃ¡sico de un estado entrelazado es el estado de Bell, que es una superposiciÃ³n de dos estados cuÃ¡nticos base:

  *|ÏˆABâŸ©=12â€“âˆš(|00âŸ©+|11âŸ©)*

  En este caso, si se efectÃºa una medida en uno de los subsistemas, el estado del otro subsistema queda instantÃ¡neamente determinado, mostrando una correlaciÃ³n perfecta entre A y B.

- Otro ejemplo es en la termodinÃ¡mica, donde la entropÃ­a clÃ¡sica mide el desorden o la incertidumbre en un sistema. De manera similar, la entropÃ­a de entrelazamiento puede interpretarse como una medida de la incertidumbre o la correlaciÃ³n intrÃ­nseca entre los subsistemas en un estado cuÃ¡ntico.
  
  Un aspecto fascinante de la entropÃ­a de entrelazamiento es su comportamiento en fases crÃ­ticas de la materia. En fÃ­sica de la materia condensada, se ha observado que la entropÃ­a de entrelazamiento puede revelar informaciÃ³n sobre transiciones de fase cuÃ¡ntica y la estructura de correlaciÃ³n en sistemas de muchos cuerpos.

  AdemÃ¡s, en el contexto de la gravitaciÃ³n cuÃ¡ntica y la teorÃ­a de cuerdas, la entropÃ­a de entrelazamiento juega un papel crucial en la comprensiÃ³n de la dinÃ¡mica de agujeros negros y la holografÃ­a. Por ejemplo, el principio hologrÃ¡fico sugiere que toda la informaciÃ³n contenida en un volumen de espacio puede ser descrita por una teorÃ­a que reside en su frontera. En estos estudios, la entropÃ­a de entrelazamiento es esencial para entender cÃ³mo se almacena y se transfiere la informaciÃ³n.

---
**Referencias:** 

1. https://modern-physics.org/entropia-de-entrelazamiento-vision-general-y-significado/ 

2. https://modern-physics.org/entropia-de-entrelazamiento-vision-general-y-significado/
   
---

# Segunda Ley de la TermodinÃ¡mica

La Segunda Ley introduce el concepto de **entropÃ­a** y establece la direcciÃ³n natural de los procesos, en esta , diversos autores han hecho aportes a la intepretacion de esta ley, las cuales permitireon llegar hasta la intepretacion actual.

# Definiciones de EntropÃ­a segÃºn diferentes autores

La entropÃ­a es uno de los conceptos mÃ¡s importantes y complejos de la termodinÃ¡mica. A lo largo de la historia, diferentes autores la han definido segÃºn el enfoque teÃ³rico utilizado: termodinÃ¡mico clÃ¡sico, estadÃ­stico, fÃ­sico-quÃ­mico o ingenieril. A continuaciÃ³n se presentan las principales definiciones, ampliadas y ordenadas cronolÃ³gicamente.

---

## 1. Rudolf Clausius (1850â€“1865)
**Padre del concepto de entropÃ­a.**  
Clausius introdujo el tÃ©rmino entropÃ­a y la relacionÃ³ con la energÃ­a tÃ©rmica no disponible para realizar trabajo.

**DefiniciÃ³n:**
> "La entropÃ­a es una magnitud del estado que describe la parte de la energÃ­a que no puede transformarse en trabajo mecÃ¡nico".

AdemÃ¡s, estableciÃ³ la famosa relaciÃ³n:

                                                        ð‘‘ð‘†=ð›¿ð‘„rev/ð‘‡

Clausius tambiÃ©n formulÃ³ la expresiÃ³n:
> *"La entropÃ­a del universo tiende a un mÃ¡ximo."*

Esta fue la primera articulaciÃ³n clara de la irreversibilidad de los procesos naturales.

---

## 2. Ludwig Boltzmann (1877)
Boltzmann dio a la entropÃ­a una interpretaciÃ³n estadÃ­stica profunda.

**DefiniciÃ³n:**
> "La entropÃ­a es una medida del nÃºmero de microestados accesibles por un sistema".

                                                         ð‘†=ð‘˜lnâ¡Î©

Donde:
- \( \Omega \) es el nÃºmero de microestados compatibles con un macroestado,
- \( k \) es la constante de Boltzmann.

**Aporte clave:**  
Introduce la idea de que la entropÃ­a estÃ¡ relacionada con el **desorden molecular**, dando una base microscÃ³pica a la termodinÃ¡mica.

---

## 3. J. Willard Gibbs (1902)
Gibbs extendiÃ³ y generalizÃ³ la definiciÃ³n estadÃ­stica de la entropÃ­a.

**DefiniciÃ³n:**
> "La entropÃ­a es una funciÃ³n que depende de la probabilidad de los microestados y mide la distribuciÃ³n de energÃ­a en un sistema".

                                                        ð‘†=âˆ’ð‘˜âˆ‘ð‘ð‘–lnð‘ð‘–
													   
Es fundamental para describir sistemas con microestados no equiprobables.

---

## 4. Max Planck (1900â€“1917)
Planck utilizÃ³ la entropÃ­a como fundamento para su formulaciÃ³n de la teorÃ­a cuÃ¡ntica.

**DefiniciÃ³n:**
> "La entropÃ­a es una funciÃ³n que determina la direcciÃ³n natural de los procesos y cuya variaciÃ³n describe la irreversibilidad".

Para Ã©l, la entropÃ­a era la clave para entender la naturaleza del equilibrio y la radiaciÃ³n.

---

## 5. Peter Atkins (2014)
Atkins modernizÃ³ el concepto para facilitar su comprensiÃ³n en quÃ­mica fÃ­sica.

**DefiniciÃ³n:**
> "La entropÃ­a es una medida de la dispersiÃ³n de la energÃ­a y del modo en que se distribuye en un sistema".

**Aporte clave:**  
Sustituye el concepto tradicional de â€œdesordenâ€ por **dispersiÃ³n de energÃ­a**, evitando interpretaciones ambiguas.

---

## 6. P. W. Bridgman (1961)
Bridgman, premio Nobel, enfatizÃ³ el carÃ¡cter experimental y operacional.

**DefiniciÃ³n:**
> "La entropÃ­a es una medida de la irreversibilidad de un proceso y del desgaste energÃ©tico inevitable que limita el trabajo Ãºtil".

Describe la entropÃ­a en tÃ©rminos de la imposibilidad prÃ¡ctica de revertir un proceso real.

---

## 7. Herbert Callen (1985)
Callen ofreciÃ³ una formulaciÃ³n matemÃ¡tica rigurosa de la termodinÃ¡mica.

**DefiniciÃ³n:**
> "La entropÃ­a es la funciÃ³n que establece la estructura de equilibrio de la termodinÃ¡mica y determina la direcciÃ³n de la evoluciÃ³n espontÃ¡nea".

Para Callen, la entropÃ­a es la **funciÃ³n fundamental del equilibrio**.

---

## 8. Smith, Van Ness y Abbott (TermodinÃ¡mica QuÃ­mica, 2005)
Autores clave para ingenierÃ­a quÃ­mica.

**DefiniciÃ³n:**
> "La entropÃ­a es una propiedad que describe el grado de dispersiÃ³n de la energÃ­a y el nÃºmero de configuraciones moleculares accesibles".

Integra el enfoque energÃ©tico y estadÃ­stico en una sola definiciÃ³n.

---

## 9. Ã‡engel y Boles (IngenierÃ­a, 2015)
Muy usado en ingenierÃ­a mecÃ¡nica y quÃ­mica.

**DefiniciÃ³n:**
> "La entropÃ­a es una medida cuantitativa del desorden molecular y del grado de irreversibilidad asociado a un proceso".

Destaca la conexiÃ³n entre entropÃ­a, aleatoriedad molecular e irreversibilidad.

---

## 10. Ilya Prigogine (Sistemas alejados del equilibrio)
Premio Nobel por estudios sobre termodinÃ¡mica del no equilibrio.

**DefiniciÃ³n:**
> "La entropÃ­a describe la producciÃ³n de irreversibilidad en sistemas alejados del equilibrio y su tendencia natural hacia nuevas estructuras o estados".

Fue pionero en entender cÃ³mo sistemas complejos pueden generar orden mientras aumenta la entropÃ­a global.

---
# EntropÃ­a del Universo, del Sistema y del Entorno

La entropÃ­a es una propiedad termodinÃ¡mica fundamental para entender la direcciÃ³n natural de los procesos y la irreversibilidad. Para analizar un fenÃ³meno se divide el universo en dos partes:

- **Sistema:** la porciÃ³n del universo que se estudia.
- **Entorno (o alrededores):** todo lo que rodea al sistema.

La suma de ambos constituye el **universo termodinÃ¡mico**.

---

# 1. EntropÃ­a del Universo

La entropÃ­a total del universo se define como la suma de la entropÃ­a del sistema y la del entorno:


                                             Î”S universo = Î”S sistema + Î”S entorno

De acuerdo con la **segunda ley de la termodinÃ¡mica**:

- **Procesos espontÃ¡neos:**
   
                                                     Î”S universo > 0

- **Procesos reversibles:**
  
                                                     Î”S universo = 0

- **Procesos imposibles fÃ­sicamente:**
  
                                                     Î”S universo < 0


### InterpretaciÃ³n:
La entropÃ­a del universo mide la **irreversibilidad global**.  
Toda fricciÃ³n, resistencia, mezcla, disipaciÃ³n de calor o turbulencia incrementa la entropÃ­a del universo.

Por eso afirmamos que:

> **La entropÃ­a del universo siempre aumenta para cualquier proceso real.**

---

# 2. EntropÃ­a del Sistema

El sistema es la parte que se analiza: un gas, una sustancia, una reacciÃ³n quÃ­mica, un motor, etc.

La entropÃ­a del sistema varÃ­a segÃºn los cambios internos de energÃ­a y el estado termodinÃ¡mico.

### Para procesos reversibles:

                                                      dS sistema = Î´Qrev/T

### Para cambios de estado a temperatura constante:

                                                      dS sistema = Qrev/T

### Propiedades importantes:

- El sistema **puede ganar o perder entropÃ­a**.
- La entropÃ­a del sistema **no determina la espontaneidad** por sÃ­ sola.
- Depende del estado interno, del volumen, temperatura y microestados accesibles.

### InterpretaciÃ³n:
La entropÃ­a del sistema mide:

- El **grado de dispersiÃ³n** de la energÃ­a interna.  
- El **nÃºmero de configuraciones microscÃ³picas** posibles.  
- El **nivel de desorden molecular**.

---

# 3. EntropÃ­a del Entorno (o Alrededores)

En la mayorÃ­a de anÃ¡lisis, el entorno se modela como un **reservorio tÃ©rmico** grande cuya temperatura no cambia.

Si el sistema intercambia calor Q con el entorno:

### Si el sistema recibe calor:

                                                        S = -Q / T entorno

### Si el sistema pierde calor:

                                                        S = +Q / T entorno

### InterpretaciÃ³n:
La entropÃ­a del entorno representa cÃ³mo afecta el proceso al resto del universo.  
Es siempre opuesta al cambio de entropÃ­a del sistema porque el calor ganado por uno es perdido por el otro.

---

# 4. Ejemplo: FusiÃ³n del hielo

**Sistema:** hielo que se derrite.  
**Entorno:** ambiente.

1. El sistema (hielo) **absorbe calor**:  
   \[
   \Delta S_{\text{sistema}} > 0
   \]

2. El entorno **pierde calor**:  
   \[
   \Delta S_{\text{entorno}} < 0
   \]

3. El aumento del sistema es mayor que la disminuciÃ³n del entorno:  
   \[
   \Delta S_{\text{universo}} > 0
   \]

**Resultado:** el proceso es espontÃ¡neo.

---
